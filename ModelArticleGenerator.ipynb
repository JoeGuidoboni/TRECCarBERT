{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelArticleGenerator",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9hEojr61Gqd"
      },
      "source": [
        "The purpose of this notebook is to generate articles paragraph by paragraph using the trained models. The articles will be generated using the existing paragraphs from the articles themselves, continuously evaluating and adding paragraphs. The query that is passed to the model is a tokenized version of the of the article that has been built up to that point, starting with the first paragraph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4VuSRib0c1t",
        "outputId": "efd5ef5f-7f2f-4c9d-fccb-1ba6ecfc5baa"
      },
      "source": [
        "! pip install trec-car-tools transformers\n",
        "import torch\n",
        "import sys\n",
        "from torch.nn import functional, CosineEmbeddingLoss, BCEWithLogitsLoss, CosineSimilarity, BCELoss\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from trec_car import read_data\n",
        "from cbor import *\n",
        "from transformers import BertModel, BertTokenizer, BertForNextSentencePrediction, Trainer, TrainingArguments\n",
        "import random\n",
        "# model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "file = 'fold-0-train.pages.cbor'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: trec-car-tools in /usr/local/lib/python3.7/dist-packages (2.5.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from trec-car-tools) (1.19.5)\n",
            "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from trec-car-tools) (1.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joEs5AgiZauA"
      },
      "source": [
        "pages = read_data.iter_pages(open(file, 'rb'))\n",
        "page = next(pages)\n",
        "my_model = BertModel.from_pretrained('drive/MyDrive/thesis_models/bert/checkpoint-5928')\n",
        "bertmodel = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIKbYP5X1F9W"
      },
      "source": [
        "# my model\n",
        "corpus = page.get_text().split('\\n')\n",
        "cos = CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "\n",
        "output_file = 'out-batch-1.txt'\n",
        "query = corpus.pop(0) # first para\n",
        "out_text = query\n",
        "\n",
        "pmodel_vals = []\n",
        "\n",
        "# run each of the paragraphs through the model and save them\n",
        "for p in corpus:\n",
        "    pemb = tokenizer(p, return_tensors='pt', padding='max_length', max_length=128, truncation=True)['input_ids']\n",
        "    pmodel = model(pemb)[0]\n",
        "    pmodel_vals.append(pmodel)\n",
        "    \n",
        "# for each paragraph, compare it to the query and find the closest\n",
        "while len(corpus) != 0:\n",
        "  qemb = tokenizer(query, return_tensors='pt', padding='max_length', max_length=128, truncation=True)['input_ids']\n",
        "  qout = model(qemb)[0]\n",
        "  cos_vals = []\n",
        "  \n",
        "  for p in pmodel_vals:\n",
        "\n",
        "    sim = cos(qout[0].type(torch.FloatTensor), p[0].type(torch.FloatTensor)) # similarity between the two output tensors\n",
        "    cos_vals.append(sim.mean())\n",
        "\n",
        "  next_para = corpus.pop(cos_vals.index(max(cos_vals)))\n",
        "  pmodel_vals.pop(cos_vals.index(max(cos_vals)))\n",
        "  query = query + next_para\n",
        "  out_text = out_text + '\\n' + next_para\n",
        "\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "   print(out_text, file = f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqoJ5-qY-dKG"
      },
      "source": [
        "# bert\n",
        "\n",
        "corpus = page.get_text().split('\\n')\n",
        "cos = CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "\n",
        "bert_file = 'bert_out.txt'\n",
        "query = corpus.pop(0) # first para\n",
        "out_text = query\n",
        "\n",
        "pmodel_vals = []\n",
        "\n",
        "# run each of the paragraphs through the model and save them\n",
        "for p in corpus:\n",
        "    pemb = tokenizer(p, return_tensors='pt', padding='max_length', max_length=128, truncation=True)['input_ids']\n",
        "    pmodel = model(pemb)[0]\n",
        "    pmodel_vals.append(pmodel)\n",
        "    \n",
        "# for each paragraph, compare it to the query and find the closest\n",
        "while len(corpus) != 0:\n",
        "  qemb = tokenizer(query, return_tensors='pt', padding='max_length', max_length=128, truncation=True)['input_ids']\n",
        "  qout = model(qemb)[0]\n",
        "  cos_vals = []\n",
        "  \n",
        "  for p in pmodel_vals:\n",
        "\n",
        "    sim = cos(qout[0].type(torch.FloatTensor), p[0].type(torch.FloatTensor)) # similarity between the two output tensors\n",
        "    cos_vals.append(sim.mean())\n",
        "\n",
        "  next_para = corpus.pop(cos_vals.index(max(cos_vals)))\n",
        "  pmodel_vals.pop(cos_vals.index(max(cos_vals)))\n",
        "  query = query + next_para\n",
        "  out_text = out_text + '\\n' + next_para\n",
        "\n",
        "with open(bert_file, 'w') as f:\n",
        "   print(out_text, file = f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYWnLwx0V6_B"
      },
      "source": [
        "#random baseline\n",
        "corpus = page.get_text().split('\\n')\n",
        "cos = CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "\n",
        "random_file = 'random_out.txt'\n",
        "query = corpus.pop(0) # first para\n",
        "\n",
        "# for each paragraph, compare it to the query and find the closest\n",
        "while len(corpus) != 0:\n",
        "  next_para = corpus.pop(random.randrange(len(corpus)))\n",
        "  query = query + '\\n' + next_para\n",
        "\n",
        "with open(random_file, 'w') as f:\n",
        "   print(query, file = f)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhzDN6-D-Vcm",
        "outputId": "a3d3f6c0-00b5-4ee7-bceb-fa630d9fe744"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.0009)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}